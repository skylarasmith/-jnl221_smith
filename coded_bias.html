<!-- This is an HTML page for the JNL221 class' first repository.-->

<!DOCTYPE html>
<html>
	<head>
	
		<meta charset="utf-8">
		<title>First repository</title>
		<link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,400,300,600,700&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
		<link href='https://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>
	
		<!-- This is where this page's style sheet is defined. -->
		<link type="text/css" rel="stylesheet" href="index.css" />

	</head>
	<body>
	
		<!-- This is the header row for this page. -->
		<div id="intro">
			<h1>SKYLAR SMITH</h1>
			<h4>Syracuse University, Fall 2025</h4>
		</div>

		<section>
			<p>The documentary “Coded Bias” opened my eyes to the prevalent influence prejudice has on artificial intelligence, and therefore, our ever-growing society. Data is embedded in the past, and history dwells within our algorithms. Therefore, algorithms, though they are claimed to be the objective truth, can be destructive and harmful, as the power belongs to those who own the code. “Coded Bias” explores multiple examples where mathematics is used as a shield for corrupt practices and how our view of the world is being governed by AI. AI has had wide scale social implications, as it is used to determine jobs and housing. If this is not being checked, “it could propagate the bias so many people put their lives on the line to fight against.” For instance, a biometric facial identification security system in London had incorrectly identified an innocent person as wanted 98% of the time. This flaw is not only rooted inaccuracy, but racial and gender discrimination. The documentary discusses how Amazon’s AI recruiting tool rejected all applications from women and that the ACLU found that 28 lawmakers were matched with the mugshots of people who were arrested. These examples taught me what an impact profiling has on AI, and more so, on the lives of those in marginalized communities.</p>The documentary “Coded Bias” opened my eyes to the prevalent influence prejudice has on artificial intelligence, and therefore, our ever-growing society. Data is embedded in the past, and history dwells within our algorithms. Therefore, algorithms, though they are claimed to be the objective truth, can be destructive and harmful, as the power belongs to those who own the code. “Coded Bias” explores multiple examples where mathematics is used as a shield for corrupt practices and how our view of the world is being governed by AI. AI has had wide scale social implications, as it is used to determine jobs and housing. If this is not being checked, “it could propagate the bias so many people put their lives on the line to fight against.” For instance, a biometric facial identification security system in London had incorrectly identified an innocent person as wanted 98% of the time. This flaw is not only rooted inaccuracy, but racial and gender discrimination. The documentary discusses how Amazon’s AI recruiting tool rejected all applications from women and that the ACLU found that 28 lawmakers were matched with the mugshots of people who were arrested. These examples taught me what an impact profiling has on AI, and more so, on the lives of those in marginalized communities.</p>Eva Constantaras & Anastasia Valeeva’s piece argues that data journalism is strongest when it begins with a hypothesis. This database consists of examples in which algorithms have been inaccurate or discriminatory against marginalized groups. While looking at this database, I noticed that the data was posted in the 1980s-2010s. If I were interviewing the dataset, I would ask the last time this information was updated. This would give me a better understanding of whether the information is relevant and newsworthy, two important factors to consider when developing a story. In addition, it would help me make connections to current issues that are similar to the incident or could build off of it. I would also ask what sources the dataset is getting its information from to make sure it is unbiased and accurate. More specifically, I would ask why the dataset believes all of those incidents are linked together and how that information brings a larger, related issue to light. One hypothesis I have created out of this dataset is that: Facial recognition algorithms misidentify individuals from marginalized communities at a higher rate than they correctly do. I could either prove or disprove this statement using the multiple incidents within the dataset that touch on this topic.
		</section>

		<div id="end">
			<h4>this page was published on github pages. fonts: montserrat, open sans.</h4>
		</div>


	</body>
</html>